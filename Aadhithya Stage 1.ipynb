{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvcc --version\n",
    "# import torch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, n=5):\n",
    "        self.n = n\n",
    "        self.B = (n - 1, n - 1) \n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "        self.action_space = len(self.actions)\n",
    "\n",
    "    def get_starting_locations(self):\n",
    "        agent_row = np.random.randint(self.n)\n",
    "        agent_col = np.random.randint(self.n)\n",
    "        package_row = np.random.randint(self.n)\n",
    "        package_col = np.random.randint(self.n)\n",
    "        while (agent_row, agent_col) == self.B or (package_row, package_col) == self.B:\n",
    "            agent_row = np.random.randint(self.n)\n",
    "            package_row = np.random.randint(self.n)\n",
    "            package_col = np.random.randint(self.n)\n",
    "        return agent_row, agent_col, package_row, package_col\n",
    "\n",
    "    def get_next_location(self, agent_row, agent_col, action_index):\n",
    "        new_row, new_col = agent_row, agent_col\n",
    "        action = self.actions[action_index]\n",
    "        if action == 'up' and agent_row > 0:\n",
    "            new_row -= 1\n",
    "        elif action == 'right' and agent_col < self.n - 1:\n",
    "            new_col += 1\n",
    "        elif action == 'down' and agent_row < self.n - 1:\n",
    "            new_row += 1\n",
    "        elif action == 'left' and agent_col > 0:\n",
    "            new_col -= 1\n",
    "        return new_row, new_col\n",
    "\n",
    "    def is_terminal_state(self, agent_row, agent_col, carrying):\n",
    "        return (agent_row, agent_col) == self.B and carrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, grid_world, learning_rate=0.1, discount_factor=0.95, epsilon=0.9):\n",
    "        self.grid_world = grid_world\n",
    "        self.q_values = np.random.rand(grid_world.n, grid_world.n, grid_world.n, grid_world.n, 2, grid_world.action_space)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.rewards = {\n",
    "            'delivery': 80,\n",
    "            'move': -1,\n",
    "            'pickup': 20\n",
    "        }\n",
    "\n",
    "    def get_next_action(self, agent_row, agent_col, package_row, package_col, carrying):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.argmax(self.q_values[agent_row, agent_col, package_row, package_col, carrying])\n",
    "        else:\n",
    "            return np.random.randint(self.grid_world.action_space)\n",
    "\n",
    "    def update_q_values(self, old_state, action_index, reward, new_state):\n",
    "        old_q_value = self.q_values[old_state][action_index]\n",
    "        temporal_difference = reward + (self.discount_factor * np.max(self.q_values[new_state])) - old_q_value\n",
    "        self.q_values[old_state][action_index] = old_q_value + (self.learning_rate * temporal_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, agent, grid_world, num_episodes=100000, max_steps_per_episode=200):\n",
    "        self.agent = agent\n",
    "        self.grid_world = grid_world\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            agent_row, agent_col, package_row, package_col = self.grid_world.get_starting_locations()\n",
    "            carrying = 0\n",
    "\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                action_index = self.agent.get_next_action(agent_row, agent_col, package_row, package_col, carrying)\n",
    "                new_agent_row, new_agent_col = self.grid_world.get_next_location(agent_row, agent_col, action_index)\n",
    "\n",
    "                if (new_agent_row, new_agent_col) == (package_row, package_col) and not carrying:\n",
    "                    reward = self.agent.rewards['pickup']\n",
    "                    carrying = 1\n",
    "                elif (new_agent_row, new_agent_col) == self.grid_world.B and carrying:\n",
    "                    reward = self.agent.rewards['delivery']\n",
    "                else:\n",
    "                    reward = self.agent.rewards['move']\n",
    "\n",
    "                old_state = (agent_row, agent_col, package_row, package_col, carrying)\n",
    "                new_state = (new_agent_row, new_agent_col, package_row, package_col, carrying)\n",
    "                self.agent.update_q_values(old_state, action_index, reward, new_state)\n",
    "\n",
    "                agent_row, agent_col = new_agent_row, new_agent_col\n",
    "\n",
    "                if self.grid_world.is_terminal_state(agent_row, agent_col, carrying):\n",
    "                    break\n",
    "        print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingManager:\n",
    "    def __init__(self, agent, grid_world, max_steps_per_episode=200):\n",
    "        self.agent = agent\n",
    "        self.grid_world = grid_world\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def test_agent(self, num_tests=10):\n",
    "        success_count = 0\n",
    "        for _ in range(num_tests):\n",
    "            agent_row, agent_col, package_row, package_col = self.grid_world.get_starting_locations()\n",
    "            carrying = 0\n",
    "            path = [(agent_row, agent_col)]\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                action_index = self.agent.get_next_action(agent_row, agent_col, package_row, package_col, carrying)\n",
    "                agent_row, agent_col = self.grid_world.get_next_location(agent_row, agent_col, action_index)\n",
    "                path.append((agent_row, agent_col))\n",
    "                if (agent_row, agent_col) == (package_row, package_col) and not carrying:\n",
    "                    carrying = 1\n",
    "                if self.grid_world.is_terminal_state(agent_row, agent_col, carrying):\n",
    "                    success_count += 1\n",
    "                    print('Success')\n",
    "                    break\n",
    "            print(f'Path taken by agent for package location: {(package_row, package_col)} - ')\n",
    "            print(path)\n",
    "        print(f'Success rate: {success_count}/{num_tests}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Success\n",
      "Path taken by agent for package location: (2, 1) - \n",
      "[(0, 3), (1, 3), (2, 3), (2, 2), (1, 2), (2, 2), (2, 1), (2, 2), (3, 2), (3, 3), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (0, 3) - \n",
      "[(0, 3), (0, 3), (1, 3), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (1, 4) - \n",
      "[(2, 2), (2, 3), (2, 4), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (2, 0) - \n",
      "[(1, 3), (1, 2), (1, 1), (1, 0), (2, 0), (3, 0), (3, 1), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (2, 2) - \n",
      "[(1, 0), (1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (0, 0) - \n",
      "[(0, 0), (0, 0), (0, 1), (1, 1), (1, 2), (2, 2), (2, 1), (2, 2), (2, 3), (2, 2), (2, 3), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (3, 1) - \n",
      "[(2, 4), (2, 3), (2, 2), (2, 1), (3, 1), (3, 2), (4, 2), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (1, 0) - \n",
      "[(1, 1), (1, 0), (1, 1), (1, 2), (2, 2), (3, 2), (4, 2), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (1, 4) - \n",
      "[(1, 4), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (4, 0) - \n",
      "[(4, 2), (4, 1), (4, 0), (4, 1), (4, 2), (4, 1), (4, 0), (4, 1), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]\n",
      "Success rate: 10/10\n"
     ]
    }
   ],
   "source": [
    "grid_world = GridWorld()\n",
    "agent = QLearningAgent(grid_world)\n",
    "trainer = Training(agent, grid_world)\n",
    "trainer.train()\n",
    "\n",
    "tester = TestingManager(agent, grid_world)\n",
    "tester.test_agent(num_tests=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
