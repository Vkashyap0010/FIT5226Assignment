{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld Class\n",
    "\n",
    "The `GridWorld` class represents a grid environment where an agent can navigate to perform tasks. It includes methods for initializing the environment, getting starting locations, determining the next location based on an action, and checking if the state is terminal.\n",
    "\n",
    "- `__init__(self, n=5)`: Initializes the grid world with size `n` and defines possible actions.\n",
    "- `get_starting_locations(self)`: Returns random starting locations for the agent and package.\n",
    "- `get_next_location(self, agent_row, agent_col, action_index)`: Computes the next location of the agent based on the given action.\n",
    "- `is_terminal_state(self, agent_row, agent_col, carrying)`: Checks if the current state is terminal (i.e., if the agent is at the base and carrying the package).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, n=5):\n",
    "        self.n = n\n",
    "        self.B = (n - 1, n - 1) \n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "        self.action_space = len(self.actions)\n",
    "\n",
    "    def get_starting_locations(self):\n",
    "        agent_row = np.random.randint(self.n)\n",
    "        agent_col = np.random.randint(self.n)\n",
    "        package_row = np.random.randint(self.n)\n",
    "        package_col = np.random.randint(self.n)\n",
    "        while (agent_row, agent_col) == self.B or (package_row, package_col) == self.B:\n",
    "            agent_row = np.random.randint(self.n)\n",
    "            package_row = np.random.randint(self.n)\n",
    "            package_col = np.random.randint(self.n)\n",
    "        return agent_row, agent_col, package_row, package_col\n",
    "\n",
    "    def get_next_location(self, agent_row, agent_col, action_index):\n",
    "        new_row, new_col = agent_row, agent_col\n",
    "        action = self.actions[action_index]\n",
    "        if action == 'up' and agent_row > 0:\n",
    "            new_row -= 1\n",
    "        elif action == 'right' and agent_col < self.n - 1:\n",
    "            new_col += 1\n",
    "        elif action == 'down' and agent_row < self.n - 1:\n",
    "            new_row += 1\n",
    "        elif action == 'left' and agent_col > 0:\n",
    "            new_col -= 1\n",
    "        return new_row, new_col\n",
    "\n",
    "    def is_terminal_state(self, agent_row, agent_col, carrying):\n",
    "        return (agent_row, agent_col) == self.B and carrying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningAgent Class\n",
    "\n",
    "The `QLearningAgent` class implements a Q-learning agent to interact with the `GridWorld` environment. It manages Q-values and updates them based on actions taken, rewards received, and future states.\n",
    "\n",
    "- `__init__(self, grid_world, learning_rate=0.1, discount_factor=0.95, epsilon=0.9)`: Initializes the Q-learning agent with the given parameters.\n",
    "- `get_next_action(self, agent_row, agent_col, package_row, package_col, carrying)`: Chooses the next action based on the epsilon-greedy policy.\n",
    "- `update_q_values(self, old_state, action_index, reward, new_state)`: Updates the Q-values using the Q-learning algorithm.\n",
    "- `Reward structure is set as`: For Delivery : 50, For a wrong move : -1, and for pickup : 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, grid_world, learning_rate=0.1, discount_factor=0.95, epsilon=0.9):\n",
    "        self.grid_world = grid_world\n",
    "        self.q_values = np.random.rand(grid_world.n, grid_world.n, grid_world.n, grid_world.n, 2, grid_world.action_space)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.rewards = {\n",
    "            'delivery': 80,\n",
    "            'move': -1,\n",
    "            'pickup': 20\n",
    "        }\n",
    "\n",
    "    def get_next_action(self, agent_row, agent_col, package_row, package_col, carrying):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.argmax(self.q_values[agent_row, agent_col, package_row, package_col, carrying])\n",
    "        else:\n",
    "            return np.random.randint(self.grid_world.action_space)\n",
    "\n",
    "    def update_q_values(self, old_state, action_index, reward, new_state):\n",
    "        old_q_value = self.q_values[old_state][action_index]\n",
    "        temporal_difference = reward + (self.discount_factor * np.max(self.q_values[new_state])) - old_q_value\n",
    "        self.q_values[old_state][action_index] = old_q_value + (self.learning_rate * temporal_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Class\n",
    "\n",
    "The `Training` class handles the training process for the Q-learning agent. It involves running multiple episodes where the agent learns from interactions with the `GridWorld` environment.\n",
    "\n",
    "- `__init__(self, agent, grid_world, num_episodes=100000, max_steps_per_episode=200)`: Initializes the training with the agent, grid world, number of episodes, and maximum steps per episode.\n",
    "- `train(self)`: Runs the training loop for the specified number of episodes, updating Q-values based on the agent's actions and rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, agent, grid_world, num_episodes=100000, max_steps_per_episode=200):\n",
    "        self.agent = agent\n",
    "        self.grid_world = grid_world\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            agent_row, agent_col, package_row, package_col = self.grid_world.get_starting_locations()\n",
    "            carrying = 0\n",
    "\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                action_index = self.agent.get_next_action(agent_row, agent_col, package_row, package_col, carrying)\n",
    "                new_agent_row, new_agent_col = self.grid_world.get_next_location(agent_row, agent_col, action_index)\n",
    "\n",
    "                if (new_agent_row, new_agent_col) == (package_row, package_col) and not carrying:\n",
    "                    reward = self.agent.rewards['pickup']\n",
    "                    carrying = 1\n",
    "                elif (new_agent_row, new_agent_col) == self.grid_world.B and carrying:\n",
    "                    reward = self.agent.rewards['delivery']\n",
    "                else:\n",
    "                    reward = self.agent.rewards['move']\n",
    "\n",
    "                old_state = (agent_row, agent_col, package_row, package_col, carrying)\n",
    "                new_state = (new_agent_row, new_agent_col, package_row, package_col, carrying)\n",
    "                self.agent.update_q_values(old_state, action_index, reward, new_state)\n",
    "\n",
    "                agent_row, agent_col = new_agent_row, new_agent_col\n",
    "\n",
    "                if self.grid_world.is_terminal_state(agent_row, agent_col, carrying):\n",
    "                    break\n",
    "        print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Class\n",
    "\n",
    "The `Testing` class evaluates the performance of the trained Q-learning agent by testing it in the `GridWorld` environment and checking its success rate.\n",
    "\n",
    "- `__init__(self, agent, grid_world, max_steps_per_episode=200)`: Initializes the testing with the agent, grid world, and maximum steps per episode.\n",
    "- `test_agent(self, num_tests=10)`: Tests the agent over a specified number of trials and prints the success rate along with the paths taken during the tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Testing:\n",
    "    def __init__(self, agent, grid_world, max_steps_per_episode=200):\n",
    "        self.agent = agent\n",
    "        self.grid_world = grid_world\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def test_agent(self, num_tests=10):\n",
    "        success_count = 0\n",
    "        for _ in range(num_tests):\n",
    "            agent_row, agent_col, package_row, package_col = self.grid_world.get_starting_locations()\n",
    "            carrying = 0\n",
    "            path = [(agent_row, agent_col)]\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                action_index = self.agent.get_next_action(agent_row, agent_col, package_row, package_col, carrying)\n",
    "                agent_row, agent_col = self.grid_world.get_next_location(agent_row, agent_col, action_index)\n",
    "                path.append((agent_row, agent_col))\n",
    "                if (agent_row, agent_col) == (package_row, package_col) and not carrying:\n",
    "                    carrying = 1\n",
    "                if self.grid_world.is_terminal_state(agent_row, agent_col, carrying):\n",
    "                    success_count += 1\n",
    "                    print('Success')\n",
    "                    break\n",
    "            print(f'Path taken by agent for package location: {(package_row, package_col)} - ')\n",
    "            print(path)\n",
    "        print(f'Success rate: {success_count}/{num_tests}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Success\n",
      "Path taken by agent for package location: (2, 4) - \n",
      "[(4, 0), (4, 1), (3, 1), (3, 2), (3, 3), (3, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (1, 4) - \n",
      "[(3, 1), (3, 2), (3, 3), (2, 3), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (0, 4), (1, 4), (2, 4), (1, 4), (2, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (2, 3), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (0, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (3, 3) - \n",
      "[(3, 4), (4, 4), (3, 4), (3, 3), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (2, 3) - \n",
      "[(3, 4), (2, 4), (2, 3), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (3, 2) - \n",
      "[(3, 0), (3, 1), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (1, 1) - \n",
      "[(2, 3), (2, 2), (2, 1), (1, 1), (1, 2), (2, 2), (1, 2), (2, 2), (3, 2), (4, 2), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (0, 4) - \n",
      "[(2, 4), (1, 4), (0, 4), (1, 4), (0, 4), (0, 3), (0, 4), (0, 3), (0, 4), (1, 4), (2, 4), (2, 3), (3, 3), (4, 3), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (4, 3) - \n",
      "[(3, 1), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (0, 0) - \n",
      "[(4, 3), (4, 2), (3, 2), (2, 2), (1, 2), (0, 2), (0, 1), (0, 0), (1, 0), (0, 0), (1, 0), (1, 0), (1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (0, 1) - \n",
      "[(4, 2), (3, 2), (2, 2), (1, 2), (1, 1), (0, 1), (1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (4, 3), (4, 4)]\n",
      "Success rate: 10/10\n"
     ]
    }
   ],
   "source": [
    "grid_world = GridWorld()\n",
    "agent = QLearningAgent(grid_world)\n",
    "trainer = Training(agent, grid_world)\n",
    "trainer.train()\n",
    "\n",
    "tester = Testing(agent, grid_world)\n",
    "tester.test_agent(num_tests=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
