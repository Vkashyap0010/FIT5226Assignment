{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridWorld Class\n",
    "\n",
    "The `GridWorld` class represents the environment in which the Q-learning agent operates. It provides the functionality to initialize the grid, generate starting locations, determine the next location based on an action, and check terminal states.\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "- `__init__(self, n=5)`: This constructor initializes the grid world with the following parameters:\n",
    "  - `n`: Size of the grid. By default, the grid is `n x n` where `n=5`.\n",
    "  - `self.B`: Coordinates of the base (goal location) set to `(n - 1, n - 1)`.\n",
    "  - `self.actions`: List of possible actions the agent can take (`'up'`, `'right'`, `'down'`, `'left'`).\n",
    "  - `self.action_space`: Number of possible actions, which is the length of `self.actions`.\n",
    "\n",
    "#### Methods\n",
    "\n",
    "- `get_starting_locations(self)`: \n",
    "  - Randomly generates starting locations for both the agent and the package, ensuring that neither the agent nor the package starts at the base location.\n",
    "  - Returns the initial positions of the agent and package as `(agent_row, agent_col, package_row, package_col)`.\n",
    "\n",
    "- `get_next_location(self, agent_row, agent_col, action_index)`:\n",
    "  - Computes the next position of the agent based on the action taken.\n",
    "  - Takes into account grid boundaries to prevent invalid moves.\n",
    "  - Returns the new position `(new_row, new_col)`.\n",
    "\n",
    "- `is_terminal_state(self, agent_row, agent_col, carrying)`:\n",
    "  - Checks if the current state is terminal. The state is terminal if the agent is at the base location and is carrying the package.\n",
    "  - Returns `True` if it is a terminal state, otherwise `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self, n=5):\n",
    "        self.n = n\n",
    "        self.B = (n - 1, n - 1) \n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "        self.action_space = len(self.actions)\n",
    "\n",
    "    def get_starting_locations(self):\n",
    "        agent_row = np.random.randint(self.n)\n",
    "        agent_col = np.random.randint(self.n)\n",
    "        package_row = np.random.randint(self.n)\n",
    "        package_col = np.random.randint(self.n)\n",
    "        while (agent_row, agent_col) == self.B or (package_row, package_col) == self.B:\n",
    "            agent_row = np.random.randint(self.n)\n",
    "            package_row = np.random.randint(self.n)\n",
    "            package_col = np.random.randint(self.n)\n",
    "        return agent_row, agent_col, package_row, package_col\n",
    "\n",
    "    def get_next_location(self, agent_row, agent_col, action_index):\n",
    "        new_row, new_col = agent_row, agent_col\n",
    "        action = self.actions[action_index]\n",
    "        if action == 'up' and agent_row > 0:\n",
    "            new_row -= 1\n",
    "        elif action == 'right' and agent_col < self.n - 1:\n",
    "            new_col += 1\n",
    "        elif action == 'down' and agent_row < self.n - 1:\n",
    "            new_row += 1\n",
    "        elif action == 'left' and agent_col > 0:\n",
    "            new_col -= 1\n",
    "        return new_row, new_col\n",
    "\n",
    "    def is_terminal_state(self, agent_row, agent_col, carrying):\n",
    "        return (agent_row, agent_col) == self.B and carrying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningAgent Class\n",
    "\n",
    "The `QLearningAgent` class implements the Q-learning algorithm, which is used to train the agent to make decisions within the `GridWorld` environment. It manages Q-values, selects actions, and updates Q-values based on the agent's experiences.\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "- `__init__(self, grid_world, learning_rate=0.1, discount_factor=0.95, epsilon=0.9)`:\n",
    "  - Initializes the Q-learning agent with the following parameters:\n",
    "    - `grid_world`: An instance of the `GridWorld` class.\n",
    "    - `self.q_values`: Q-values for all state-action pairs, initialized randomly. The shape of this array is `(n, n, n, n, 2, action_space)`, where `2` corresponds to the carrying state (0 or 1).\n",
    "    - `self.learning_rate`: Learning rate \\( \\alpha \\) that controls how much new information overrides old values.\n",
    "    - `self.discount_factor`: Discount factor \\( \\gamma \\) which determines the importance of future rewards.\n",
    "    - `self.epsilon`: Epsilon for the epsilon-greedy policy, balancing exploration and exploitation.\n",
    "    - `self.rewards`: Dictionary of rewards for different actions (`'delivery'`, `'move'`, `'pickup'`).\n",
    "\n",
    "#### Methods\n",
    "\n",
    "- `get_next_action(self, agent_row, agent_col, package_row, package_col, carrying)`:\n",
    "  - Selects the next action for the agent based on the epsilon-greedy policy.\n",
    "  - With probability `epsilon`, chooses the action with the highest Q-value. Otherwise, selects a random action.\n",
    "  - Returns the index of the selected action.\n",
    "\n",
    "- `update_q_values(self, old_state, action_index, reward, new_state)`:\n",
    "  - Updates the Q-values using the Q-learning update rule.\n",
    "  - Computes the temporal difference and updates the Q-value for the given old state and action based on the reward received and the maximum Q-value for the new state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, grid_world, learning_rate=0.1, discount_factor=0.95, epsilon=0.9):\n",
    "        self.grid_world = grid_world\n",
    "        self.q_values = np.random.rand(grid_world.n, grid_world.n, grid_world.n, grid_world.n, 2, grid_world.action_space)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.rewards = {\n",
    "            'delivery': 80,\n",
    "            'move': -1,\n",
    "            'pickup': 20\n",
    "        }\n",
    "\n",
    "    def get_next_action(self, agent_row, agent_col, package_row, package_col, carrying):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.argmax(self.q_values[agent_row, agent_col, package_row, package_col, carrying])\n",
    "        else:\n",
    "            return np.random.randint(self.grid_world.action_space)\n",
    "\n",
    "    def update_q_values(self, old_state, action_index, reward, new_state):\n",
    "        old_q_value = self.q_values[old_state][action_index]\n",
    "        temporal_difference = reward + (self.discount_factor * np.max(self.q_values[new_state])) - old_q_value\n",
    "        self.q_values[old_state][action_index] = old_q_value + (self.learning_rate * temporal_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Class\n",
    "\n",
    "The `Training` class is responsible for training the Q-learning agent within the `GridWorld` environment. This process involves running multiple episodes where the agent interacts with the environment, learns from the rewards, and updates its Q-values to improve performance.\n",
    "\n",
    "#### Initialization\n",
    "\n",
    "- `__init__(self, agent, grid_world, num_episodes=100000, max_steps_per_episode=200)`: This constructor initializes the training parameters:\n",
    "  - `agent`: An instance of the `QLearningAgent` class.\n",
    "  - `grid_world`: An instance of the `GridWorld` class.\n",
    "  - `num_episodes`: The number of training episodes to run.\n",
    "  - `max_steps_per_episode`: The maximum number of steps to take per episode.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "The `train(self)` method executes the training loop, which involves:\n",
    "\n",
    "1. **Episode Loop**: Iterates through the specified number of episodes (`num_episodes`).\n",
    "   - **Starting Locations**: For each episode, the agent and package start at random locations obtained from `grid_world.get_starting_locations()`. The agent starts without carrying the package (`carrying = 0`).\n",
    "\n",
    "2. **Step Loop**: Within each episode, the agent takes actions for a maximum of `max_steps_per_episode` steps:\n",
    "   - **Action Selection**: The agent selects an action based on its policy using `self.agent.get_next_action()`. This method implements the epsilon-greedy strategy, where the agent chooses the action with the highest Q-value with probability `epsilon`.\n",
    "   - **State Transition**: The agent moves to a new location based on the selected action using `grid_world.get_next_location()`.\n",
    "\n",
    "3. **Reward Calculation**: After moving, the agent receives a reward based on its new state:\n",
    "   - **Pickup**: If the agent reaches the package location and is not carrying the package, it receives a `pickup` reward.\n",
    "   - **Delivery**: If the agent reaches the base location while carrying the package, it receives a `delivery` reward.\n",
    "   - **Move**: For all other moves, the agent receives a `move` penalty.\n",
    "\n",
    "4. **Q-Value Update**: The Q-values are updated using the Q-learning update rule. The update involves the following:\n",
    "\n",
    "   - Applies the Bellman equation for Q-learning:\n",
    "    ```\n",
    "    Q(s, a) = Q(s, a) + α * [r + γ * max_a'(Q(s', a')) - Q(s, a)]\n",
    "    ```\n",
    "    where:\n",
    "    - `Q(s, a)` is the current Q-value.\n",
    "    - `α` is the learning rate.\n",
    "    - `r` is the reward.\n",
    "    - `γ` is the discount factor.\n",
    "    - `max_a'(Q(s', a'))` is the maximum Q-value for the next state `s'` over all possible actions.\n",
    "\n",
    "   The `update_q_values()` method in the `QLearningAgent` class applies this formula to adjust the Q-values based on the observed transition from state \\( s \\) to \\( s' \\) and the reward \\( r \\).\n",
    "\n",
    "5. **Terminal State Check**: If the agent reaches a terminal state, the episode ends early.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, agent, grid_world, num_episodes=100000, max_steps_per_episode=200):\n",
    "        self.agent = agent\n",
    "        self.grid_world = grid_world\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def train(self):\n",
    "        for episode in range(self.num_episodes):\n",
    "            agent_row, agent_col, package_row, package_col = self.grid_world.get_starting_locations()\n",
    "            carrying = 0\n",
    "\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                action_index = self.agent.get_next_action(agent_row, agent_col, package_row, package_col, carrying)\n",
    "                new_agent_row, new_agent_col = self.grid_world.get_next_location(agent_row, agent_col, action_index)\n",
    "\n",
    "                if (new_agent_row, new_agent_col) == (package_row, package_col) and not carrying:\n",
    "                    reward = self.agent.rewards['pickup']\n",
    "                    carrying = 1\n",
    "                elif (new_agent_row, new_agent_col) == self.grid_world.B and carrying:\n",
    "                    reward = self.agent.rewards['delivery']\n",
    "                else:\n",
    "                    reward = self.agent.rewards['move']\n",
    "\n",
    "                old_state = (agent_row, agent_col, package_row, package_col, carrying)\n",
    "                new_state = (new_agent_row, new_agent_col, package_row, package_col, carrying)\n",
    "                self.agent.update_q_values(old_state, action_index, reward, new_state)\n",
    "\n",
    "                agent_row, agent_col = new_agent_row, new_agent_col\n",
    "\n",
    "                if self.grid_world.is_terminal_state(agent_row, agent_col, carrying):\n",
    "                    break\n",
    "        print('Training complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Class\n",
    "\n",
    "The `Testing` class evaluates the performance of the trained Q-learning agent by testing it in the `GridWorld` environment and checking its success rate.\n",
    "\n",
    "- `__init__(self, agent, grid_world, max_steps_per_episode=200)`: Initializes the testing with the agent, grid world, and maximum steps per episode.\n",
    "- `test_agent(self, num_tests=10)`: Tests the agent over a specified number of trials and prints the success rate along with the paths taken during the tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Testing:\n",
    "    def __init__(self, agent, grid_world, max_steps_per_episode=200):\n",
    "        self.agent = agent\n",
    "        self.grid_world = grid_world\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "\n",
    "    def test_agent(self, num_tests=10):\n",
    "        success_count = 0\n",
    "        for _ in range(num_tests):\n",
    "            agent_row, agent_col, package_row, package_col = self.grid_world.get_starting_locations()\n",
    "            carrying = 0\n",
    "            path = [(agent_row, agent_col)]\n",
    "            for step in range(self.max_steps_per_episode):\n",
    "                action_index = self.agent.get_next_action(agent_row, agent_col, package_row, package_col, carrying)\n",
    "                agent_row, agent_col = self.grid_world.get_next_location(agent_row, agent_col, action_index)\n",
    "                path.append((agent_row, agent_col))\n",
    "                if (agent_row, agent_col) == (package_row, package_col) and not carrying:\n",
    "                    carrying = 1\n",
    "                if self.grid_world.is_terminal_state(agent_row, agent_col, carrying):\n",
    "                    success_count += 1\n",
    "                    print('Success')\n",
    "                    break\n",
    "            print(f'Path taken by agent for package location: {(package_row, package_col)} - ')\n",
    "            print(path)\n",
    "        print(f'Success rate: {success_count}/{num_tests}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Success\n",
      "Path taken by agent for package location: (2, 4) - \n",
      "[(4, 0), (4, 1), (3, 1), (3, 2), (3, 3), (3, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (1, 4) - \n",
      "[(3, 1), (3, 2), (3, 3), (2, 3), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (0, 4), (1, 4), (2, 4), (1, 4), (2, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (2, 3), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (0, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (1, 3), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (1, 4), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (3, 3) - \n",
      "[(3, 4), (4, 4), (3, 4), (3, 3), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (2, 3) - \n",
      "[(3, 4), (2, 4), (2, 3), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (3, 2) - \n",
      "[(3, 0), (3, 1), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (1, 1) - \n",
      "[(2, 3), (2, 2), (2, 1), (1, 1), (1, 2), (2, 2), (1, 2), (2, 2), (3, 2), (4, 2), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (0, 4) - \n",
      "[(2, 4), (1, 4), (0, 4), (1, 4), (0, 4), (0, 3), (0, 4), (0, 3), (0, 4), (1, 4), (2, 4), (2, 3), (3, 3), (4, 3), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (4, 3) - \n",
      "[(3, 1), (3, 2), (3, 3), (4, 3), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (0, 0) - \n",
      "[(4, 3), (4, 2), (3, 2), (2, 2), (1, 2), (0, 2), (0, 1), (0, 0), (1, 0), (0, 0), (1, 0), (1, 0), (1, 1), (1, 2), (2, 2), (2, 3), (2, 4), (3, 4), (4, 4)]\n",
      "Success\n",
      "Path taken by agent for package location: (0, 1) - \n",
      "[(4, 2), (3, 2), (2, 2), (1, 2), (1, 1), (0, 1), (1, 1), (1, 2), (2, 2), (2, 3), (3, 3), (4, 3), (4, 4)]\n",
      "Success rate: 10/10\n"
     ]
    }
   ],
   "source": [
    "grid_world = GridWorld()\n",
    "agent = QLearningAgent(grid_world)\n",
    "trainer = Training(agent, grid_world)\n",
    "trainer.train()\n",
    "\n",
    "tester = Testing(agent, grid_world)\n",
    "tester.test_agent(num_tests=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
